{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn 最近邻域算法\n",
    "\n",
    "    将测试图片，与训练图片进行比较，如果存在k张与测试图片最近的训练图片，选取出现的概率最高的训练图片，则概率最高的图片的标签值记录下来，就是最终检测出来的结果\n",
    "    \n",
    "步骤：\n",
    "1. 准备数据\n",
    "2. 训练数据，得到：像素间距离==>累加求和==>图片距离\n",
    "3. 找到k个最接近的图片\n",
    "4. 使用标签值解析图片\n",
    "5. 将标签值转成具体的数字\n",
    "6. 概率检测统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-10a5b8daee6b>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/zhengtaizhong/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/zhengtaizhong/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/zhengtaizhong/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/zhengtaizhong/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/zhengtaizhong/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-10a5b8daee6b>:57: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "f3_res[0,0]: 92.807846\n",
      "f4_res[0,0]: -92.807846\n",
      "f7_res [[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]]\n",
      "f8_res [[0. 0. 0. 0. 0. 0. 0. 4. 0. 0.]\n",
      " [4. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 3.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 2. 0. 2.]\n",
      " [0. 0. 0. 1. 0. 3. 0. 0. 0. 0.]]\n",
      "f9_res [7 0 9 7 5]\n",
      "[7 0 9 9 5]\n",
      "准确率： 80.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random             # 生成随机数组，用来测试图片\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)    # 参数1：文件路径，参数2：one-hot编码\n",
    "\n",
    "\n",
    "# 属性设置\n",
    "trainNum = 55000     # 总共有55000张训练图片\n",
    "testNum = 10000      # 总共有10000张测试图片\n",
    "trainSize = 500      # 随机选出训练图片的数量\n",
    "testSize = 5         # 随机选出测试图片的数量\n",
    "\n",
    "\n",
    "# 随机获取训练图片(样本)和测试图片(样本)，以及相应的标签值\n",
    "trainIndex = np.random.choice(trainNum, trainSize, replace=False)   # 范围0~trainNum之间，随机选择trainSize个数字，不可重复。在0~55k中，随机选取500个数字\n",
    "testIndex = np.random.choice(testNum, testSize, replace=False)\n",
    "\n",
    "trainData = mnist.train.images[trainIndex]        # 随机选出训练图片，(500, 784)，500张图片，784=图片高宽28*28(图片上所有的像素点)\n",
    "trainLabel = mnist.train.labels[trainIndex]       # 训练图片的标签值，(500, 10)，500张图片，10个目标值(one-hot)\n",
    "\n",
    "testData = mnist.train.images[testIndex]          # 随机选出测试图片，(5, 784)\n",
    "testLabel = mnist.train.labels[testIndex]         # 测试图片的标签值，(5, 10)\n",
    "\n",
    "# 找到k个与测试图片相近的图片，并且统计这些图片中，一个类别出现次数最多的图片，则这个类别就是最终的数据\n",
    "k = 4\n",
    "\n",
    "# 准备tf的输入数据（train+test）\n",
    "trainDataInput = tf.placeholder(shape=[None, 784], dtype=tf.float32)    # 加载输入数据，每一行的784列表示一个张完整的图片，None表示图片的张数，由具体的特征值类型决定\n",
    "trainLabelInput = tf.placeholder(shape=[None, 10], dtype=tf.float32)    # 每一张图片的标签值，都是一个10列one-hot编码\n",
    "\n",
    "testDataInput = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "testLabelInput = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# knn distance：\n",
    "# 1.【训练图片】增加一维：(5,784) ==> (5,1,784)，5张图片每1张图片784个像素点。\n",
    "# 2. 计算5张【训练图片】中的每1张图片和500张【测试图片】之间，784个像素点的距离差。(5,500,784) 平面数：【测试图片数据】。行数：【训练图片数据】。列数：【两张图片距离之差】\n",
    "# 3. 降维累加：计算【5张测试图】与【500张训练图】之间的距离差值。(5,500) \n",
    "f1 = tf.expand_dims(testDataInput,1)    # 维度转换\n",
    "f2 = tf.subtract(trainDataInput,f1)     # 计算距离差\n",
    "f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)    # 把784像素点的差值(降维)累加。参数2：在哪一个维度进行数据累加\n",
    "\n",
    "# 例：(1,100) 表示，第2张测试图片，与第99张训练图片之间的所有像素的距离之差\n",
    "\n",
    "# 如何根据计算出来的距离，找到knn中k个最近的图片。\n",
    "# 在5张【测试图片】中，使用每1张训练图片在【500张训练图】中找出4个相似的图片\n",
    "f4 = tf.negative(f3)    # 取反，130==> -130\n",
    "f5,f6 = tf.nn.top_k(f4, k=4)    # 选取f4中，最大的4个数。f3最小的4个数，即最接近测试图片的4个数。f5：距离差(4个值)，f6：4张最近的训练图片的下标\n",
    "\n",
    "# 通过k个相似图的label属性，解析k个相似的图。\n",
    "f7 = tf.gather(trainLabelInput, f6)      # 获取所有标签\n",
    "\n",
    "# 数字获取\n",
    "f8 = tf.reduce_sum(f7, reduction_indices=1)    # 竖直方向，列累加\n",
    "f9 = tf.argmax(f8, dimension=1)    # 返回f8中，每行最大值的下标，即5张测试图片经过训练得出的5个标签值\n",
    "\n",
    "# 将f9与testLabel比较，如果相同，则检测成功率为100%。一半相同，则50%\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    f1_res = sess.run(f1, feed_dict={testDataInput:testData})    # (5,1,784)，注:testData = testData[0:5]左闭右开\n",
    "    f2_res = sess.run(f2, feed_dict={trainDataInput:trainData, testDataInput:testData})    # (5,500,784)，注:一个op用了n个占位符，就得传n个占位符\n",
    "    f3_res = sess.run(f3, feed_dict={trainDataInput:trainData, testDataInput:testData})    # (5,500)\n",
    "    f4_res = sess.run(f4, feed_dict={trainDataInput:trainData, testDataInput:testData})    # (5,500)\n",
    "\n",
    "    print(\"f3_res[0,0]:\", f3_res[0,0])  # 158.20392\n",
    "    print(\"f4_res[0,0]:\", f4_res[0,0])  # -158.20392\n",
    "    \n",
    "    f5_res, f6_res = sess.run((f5, f6), feed_dict={trainDataInput:trainData, testDataInput:testData})     # f5_res(5,4);f6_res(5,4)，5张测试图片的每1张测试图片对应4张训练图片\n",
    "    # f5_res是每1张测试图片到4张训练图片的距离，f6_res是每1张测试图片对应的训练图片的下标index\n",
    "#     print(\"f5_res[0,0]:\", f5_res[0,0])\n",
    "#     print(\"f6_res[0,0]:\", f6_res[0,0])\n",
    "#     print(\"f5_res[0]:\", f3_res[0])\n",
    "#     print(\"f6_res[0]:\", f3_res[0])\n",
    "    \n",
    "    f7_res = sess.run(f7, feed_dict={trainDataInput:trainData, testDataInput:testData, trainLabelInput:trainLabel})    # (5,4,10)\n",
    "    print(\"f7_res\",f7_res)\n",
    "    \n",
    "    f8_res, f9_res = sess.run((f8, f9), feed_dict={trainDataInput:trainData, testDataInput:testData, trainLabelInput:trainLabel})    # f8_res(5,10), f9_res(5,)\n",
    "    print(\"f8_res\",f8_res)\n",
    "    print(\"f9_res\",f9_res)\n",
    "    \n",
    "    label_res = np.argmax(testLabel, axis=1)\n",
    "    print(label_res)\n",
    "    \n",
    "\n",
    "# 检测概率\n",
    "j = 0\n",
    "for i in range(5): # 在5张图片的标签值中遍历\n",
    "    if label_res[i] == f9_res[i]:\n",
    "        j = j+1\n",
    "print(\"准确率：\", j*100/5)    # 小数转成百分数，5组数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-2-0386e4c45c53>:126: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "第【0】步，50个预测图片误差为【4.445383】，准确率为【0.060000】\n",
      "第【1】步，50个预测图片误差为【6.730469】，准确率为【0.120000】\n",
      "第【2】步，50个预测图片误差为【3.389718】，准确率为【0.080000】\n",
      "第【3】步，50个预测图片误差为【2.391407】，准确率为【0.080000】\n",
      "第【4】步，50个预测图片误差为【2.335293】，准确率为【0.100000】\n",
      "第【5】步，50个预测图片误差为【2.305876】，准确率为【0.040000】\n",
      "第【6】步，50个预测图片误差为【2.284229】，准确率为【0.140000】\n",
      "第【7】步，50个预测图片误差为【2.213932】，准确率为【0.160000】\n",
      "第【8】步，50个预测图片误差为【2.153918】，准确率为【0.220000】\n",
      "第【9】步，50个预测图片误差为【2.136751】，准确率为【0.280000】\n",
      "第【10】步，50个预测图片误差为【2.133242】，准确率为【0.260000】\n",
      "第【11】步，50个预测图片误差为【2.001035】，准确率为【0.220000】\n",
      "第【12】步，50个预测图片误差为【2.160360】，准确率为【0.200000】\n",
      "第【13】步，50个预测图片误差为【1.807023】，准确率为【0.520000】\n",
      "第【14】步，50个预测图片误差为【1.926786】，准确率为【0.400000】\n",
      "第【15】步，50个预测图片误差为【1.698184】，准确率为【0.500000】\n",
      "第【16】步，50个预测图片误差为【1.759662】，准确率为【0.300000】\n",
      "第【17】步，50个预测图片误差为【1.993578】，准确率为【0.380000】\n",
      "第【18】步，50个预测图片误差为【1.929626】，准确率为【0.320000】\n",
      "第【19】步，50个预测图片误差为【2.028741】，准确率为【0.380000】\n",
      "第【20】步，50个预测图片误差为【1.536875】，准确率为【0.460000】\n",
      "第【21】步，50个预测图片误差为【1.683302】，准确率为【0.460000】\n",
      "第【22】步，50个预测图片误差为【1.227578】，准确率为【0.620000】\n",
      "第【23】步，50个预测图片误差为【0.877768】，准确率为【0.740000】\n",
      "第【24】步，50个预测图片误差为【1.037201】，准确率为【0.660000】\n",
      "第【25】步，50个预测图片误差为【0.838076】，准确率为【0.760000】\n",
      "第【26】步，50个预测图片误差为【0.866916】，准确率为【0.740000】\n",
      "第【27】步，50个预测图片误差为【1.138987】，准确率为【0.580000】\n",
      "第【28】步，50个预测图片误差为【1.337001】，准确率为【0.680000】\n",
      "第【29】步，50个预测图片误差为【1.105177】，准确率为【0.720000】\n",
      "第【30】步，50个预测图片误差为【1.004869】，准确率为【0.720000】\n",
      "第【31】步，50个预测图片误差为【0.729856】，准确率为【0.780000】\n",
      "第【32】步，50个预测图片误差为【0.981658】，准确率为【0.560000】\n",
      "第【33】步，50个预测图片误差为【1.170319】，准确率为【0.740000】\n",
      "第【34】步，50个预测图片误差为【0.951928】，准确率为【0.680000】\n",
      "第【35】步，50个预测图片误差为【0.820223】，准确率为【0.680000】\n",
      "第【36】步，50个预测图片误差为【0.796724】，准确率为【0.780000】\n",
      "第【37】步，50个预测图片误差为【0.724522】，准确率为【0.740000】\n",
      "第【38】步，50个预测图片误差为【0.744046】，准确率为【0.800000】\n",
      "第【39】步，50个预测图片误差为【0.602171】，准确率为【0.780000】\n",
      "第【40】步，50个预测图片误差为【0.428483】，准确率为【0.920000】\n",
      "第【41】步，50个预测图片误差为【0.384399】，准确率为【0.860000】\n",
      "第【42】步，50个预测图片误差为【0.395920】，准确率为【0.880000】\n",
      "第【43】步，50个预测图片误差为【0.560717】，准确率为【0.900000】\n",
      "第【44】步，50个预测图片误差为【0.460921】，准确率为【0.860000】\n",
      "第【45】步，50个预测图片误差为【0.662931】，准确率为【0.840000】\n",
      "第【46】步，50个预测图片误差为【0.361360】，准确率为【0.900000】\n",
      "第【47】步，50个预测图片误差为【0.451687】，准确率为【0.840000】\n",
      "第【48】步，50个预测图片误差为【0.330988】，准确率为【0.880000】\n",
      "第【49】步，50个预测图片误差为【0.536797】，准确率为【0.800000】\n",
      "第【50】步，50个预测图片误差为【0.464370】，准确率为【0.860000】\n",
      "第【51】步，50个预测图片误差为【0.591497】，准确率为【0.800000】\n",
      "第【52】步，50个预测图片误差为【0.348892】，准确率为【0.960000】\n",
      "第【53】步，50个预测图片误差为【0.340291】，准确率为【0.880000】\n",
      "第【54】步，50个预测图片误差为【0.664783】，准确率为【0.780000】\n",
      "第【55】步，50个预测图片误差为【1.037996】，准确率为【0.680000】\n",
      "第【56】步，50个预测图片误差为【0.560457】，准确率为【0.860000】\n",
      "第【57】步，50个预测图片误差为【0.571941】，准确率为【0.840000】\n",
      "第【58】步，50个预测图片误差为【0.458149】，准确率为【0.820000】\n",
      "第【59】步，50个预测图片误差为【0.381249】，准确率为【0.900000】\n",
      "第【60】步，50个预测图片误差为【0.492859】，准确率为【0.800000】\n",
      "第【61】步，50个预测图片误差为【0.645544】，准确率为【0.800000】\n",
      "第【62】步，50个预测图片误差为【0.556863】，准确率为【0.820000】\n",
      "第【63】步，50个预测图片误差为【0.441324】，准确率为【0.840000】\n",
      "第【64】步，50个预测图片误差为【0.590702】，准确率为【0.800000】\n",
      "第【65】步，50个预测图片误差为【0.427340】，准确率为【0.860000】\n",
      "第【66】步，50个预测图片误差为【0.494742】，准确率为【0.800000】\n",
      "第【67】步，50个预测图片误差为【0.221265】，准确率为【0.940000】\n",
      "第【68】步，50个预测图片误差为【0.209392】，准确率为【0.940000】\n",
      "第【69】步，50个预测图片误差为【0.196716】，准确率为【0.960000】\n",
      "第【70】步，50个预测图片误差为【0.355846】，准确率为【0.860000】\n",
      "第【71】步，50个预测图片误差为【0.145743】，准确率为【0.960000】\n",
      "第【72】步，50个预测图片误差为【0.326989】，准确率为【0.900000】\n",
      "第【73】步，50个预测图片误差为【0.381141】，准确率为【0.920000】\n",
      "第【74】步，50个预测图片误差为【0.476100】，准确率为【0.860000】\n",
      "第【75】步，50个预测图片误差为【0.291291】，准确率为【0.940000】\n",
      "第【76】步，50个预测图片误差为【0.234717】，准确率为【0.940000】\n",
      "第【77】步，50个预测图片误差为【0.248414】，准确率为【0.900000】\n",
      "第【78】步，50个预测图片误差为【0.149311】，准确率为【0.960000】\n",
      "第【79】步，50个预测图片误差为【0.434828】，准确率为【0.840000】\n",
      "第【80】步，50个预测图片误差为【0.140093】，准确率为【0.960000】\n",
      "第【81】步，50个预测图片误差为【0.326172】，准确率为【0.860000】\n",
      "第【82】步，50个预测图片误差为【0.162604】，准确率为【0.960000】\n",
      "第【83】步，50个预测图片误差为【0.294768】，准确率为【0.920000】\n",
      "第【84】步，50个预测图片误差为【0.174135】，准确率为【0.940000】\n",
      "第【85】步，50个预测图片误差为【0.150031】，准确率为【0.960000】\n",
      "第【86】步，50个预测图片误差为【0.292495】，准确率为【0.920000】\n",
      "第【87】步，50个预测图片误差为【0.337523】，准确率为【0.900000】\n",
      "第【88】步，50个预测图片误差为【0.273678】，准确率为【0.960000】\n",
      "第【89】步，50个预测图片误差为【0.193815】，准确率为【0.940000】\n",
      "第【90】步，50个预测图片误差为【0.273360】，准确率为【0.940000】\n",
      "第【91】步，50个预测图片误差为【0.237010】，准确率为【0.940000】\n",
      "第【92】步，50个预测图片误差为【0.336092】，准确率为【0.900000】\n",
      "第【93】步，50个预测图片误差为【0.394476】，准确率为【0.880000】\n",
      "第【94】步，50个预测图片误差为【0.151543】，准确率为【0.940000】\n",
      "第【95】步，50个预测图片误差为【0.343024】，准确率为【0.880000】\n",
      "第【96】步，50个预测图片误差为【0.211830】，准确率为【0.940000】\n",
      "第【97】步，50个预测图片误差为【0.357045】，准确率为【0.880000】\n",
      "第【98】步，50个预测图片误差为【0.249096】，准确率为【0.920000】\n",
      "第【99】步，50个预测图片误差为【0.210672】，准确率为【0.960000】\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "卷积模型（确定网络结构和参数）：\n",
    "两层卷积池化和一层输出层\n",
    "\n",
    "第一层\n",
    "    卷积：32个filter、大小5*5、strides=1、padding=\"SAME\"\n",
    "    激活：Relu\n",
    "    池化：大小 2x2、strides = 2\n",
    "第二层\n",
    "    卷积：64个filter、大小5*5、strides=1、padding=\"SAME\"\n",
    "    激活：Relu\n",
    "    池化：大小 2x2、strides = 2\n",
    "全连接层\n",
    "\n",
    "重点：计算每层数据的变化\n",
    "\n",
    "梯度爆炸：\n",
    "1. 调整参数w,b的值\n",
    "2. 使用tf.train.AdamOptimizer()\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# 定义专门初始化权重和偏置的两个函数（因为两个参数的形状不同，所以要把权重和偏置分开）\n",
    "def weight_initialize(shape):\n",
    "    weight = tf.Variable(tf.random_normal(shape=shape, mean=0.0, stddev=0.1), name=\"weight\")\n",
    "    return weight\n",
    "\n",
    "\n",
    "def bias_initialize(shape):\n",
    "    bias = tf.Variable(tf.random_normal(shape=shape, mean=0.0, stddev=0.1), name=\"bias\")\n",
    "    return bias\n",
    "\n",
    "\n",
    "def cnn_model():\n",
    "    \"\"\"自定义卷积模型\"\"\"\n",
    "    # 1. 定义特征值和目标值的占位符，便于卷积计算:x[None,784], y[None,10]\n",
    "    with tf.variable_scope(\"data\"):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name=\"feature\")\n",
    "        y_true = tf.placeholder(tf.float32, [None, 10], name=\"y_true\")\n",
    "\n",
    "    # 2. 第一层：\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        # 2.1 卷积层：32个filter、大小5*5、strides=1、padding=\"SAME\"\n",
    "\n",
    "        # 初始化权重[5,5,1,32] 和 偏置[32]\n",
    "        conv1_weight = weight_initialize([5, 5, 1, 32])\n",
    "        conv1_bias = bias_initialize([32])\n",
    "\n",
    "        # 特征形状变成4维，用于卷积运算。（重点：None就是-1）\n",
    "        x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "        # 卷积运算: [-1,28,28,1] --> [-1,28,28,32]\n",
    "        conv1_x = tf.nn.conv2d(input=x_reshape,\n",
    "                               filter=conv1_weight,\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               padding=\"SAME\",\n",
    "                               name=\"conv1\") + conv1_bias\n",
    "        # 2.2 激活层运算\n",
    "        relu1_x = tf.nn.relu(conv1_x, name=\"relu1\")\n",
    "\n",
    "        # 2.3 池化层: 大小 2x2、strides=2、padding=\"SAME\"\n",
    "        # [-1,28,28,32] --> [-1,14,14,32]\n",
    "        pool1_x = tf.nn.max_pool(value=relu1_x,\n",
    "                                 ksize=[1, 2, 2, 1],\n",
    "                                 strides=[1, 2, 2, 1],\n",
    "                                 padding=\"SAME\",\n",
    "                                 name=\"pool1\")\n",
    "\n",
    "    # 3. 第二层\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        # 3.1 卷积层：64个filter、大小5*5、strides=1、padding=\"SAME\"\n",
    "\n",
    "        # 初始化权重[5,5,32,64] 和 偏置[64]\n",
    "        conv2_weight = weight_initialize([5, 5, 32, 64])\n",
    "        conv2_bias = bias_initialize([64])\n",
    "\n",
    "        # 卷积运算：[-1,14,14,32] --> [-1,14,14,64]\n",
    "        conv2_x = tf.nn.conv2d(input=pool1_x,\n",
    "                               filter=conv2_weight,\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               padding=\"SAME\",\n",
    "                               name=\"conv1\") + conv2_bias\n",
    "\n",
    "        # 3.2 激活层运算\n",
    "        relu2_x = tf.nn.relu(conv2_x, name=\"relu2\")\n",
    "\n",
    "        # 2.3 池化层: 大小 2x2、strides=2、padding=\"SAME\"\n",
    "        # [-1,14,14,64] --> [-1,7,7,64]\n",
    "        pool2_x = tf.nn.max_pool(value=relu2_x,\n",
    "                                 ksize=[1, 2, 2, 1],\n",
    "                                 strides=[1, 2, 2, 1],\n",
    "                                 padding=\"SAME\",\n",
    "                                 name=\"pool2\")\n",
    "\n",
    "    # 4. 全连接层\n",
    "    with tf.variable_scope(\"last_layer\"):\n",
    "        \"\"\"10个输出\"\"\"\n",
    "        # 初始化权重[7*7*64,10] 和 偏置[10]\n",
    "        fc_weight = weight_initialize([7 * 7 * 64, 10])\n",
    "        fc_bias = bias_initialize([10])\n",
    "\n",
    "        # 4维转换2维\n",
    "        pool2_x_reshape = tf.reshape(pool2_x, [-1, 7 * 7 * 64])\n",
    "\n",
    "        # 全连接层矩阵运算\n",
    "        y_predict = tf.matmul(pool2_x_reshape, fc_weight) + fc_bias\n",
    "\n",
    "    return x, y_true, y_predict\n",
    "\n",
    "\n",
    "def cnn_mnist():\n",
    "    \"\"\"卷积网络识别训练\"\"\"\n",
    "\n",
    "    # 1. 获取数据\n",
    "    mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)\n",
    "\n",
    "    # 2. 建立卷积网络模型：（说明）\n",
    "    x, y_true, y_predict = cnn_model()\n",
    "\n",
    "    # 3. 根据输出的结果计算其softmax，并与真实值进行交叉熵损失计算\n",
    "    with tf.variable_scope(\"softmax_crossentropy\"):\n",
    "        # 先进行网络输出值的softmax概率计算，再计算每个样本的损失\n",
    "        all_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict, name=\"compute_loss\")\n",
    "\n",
    "        # 求出平均损失\n",
    "        loss = tf.reduce_mean(all_loss)\n",
    "\n",
    "    # 4. 梯度下降优化\n",
    "    with tf.variable_scope(\"GD\"):\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss=loss)\n",
    "        # 使用adam优化器：\n",
    "        # train_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss=loss)\n",
    "\n",
    "    # 5. 计算准确率\n",
    "    with tf.variable_scope(\"accuracy\"):\n",
    "        equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))\n",
    "\n",
    "\n",
    "        # 0. 开启会话进行训练（train_op）\n",
    "        with tf.Session() as sess:\n",
    "            # 初始化变量OP\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # 循环训练\n",
    "            for i in range(100):  # 步数=100\n",
    "                # 每批次给50个样本\n",
    "                x_mnist, y_mnist = mnist.train.next_batch(50)\n",
    "                # run的feed_dict机制，指定给占位符变量传数据。只要运行的变量中含有占位符，都要进行feed_dict\n",
    "                _, loss_run, accuracy_run= sess.run([train_op, loss, accuracy],\n",
    "                                                                 feed_dict={x: x_mnist, y_true: y_mnist})\n",
    "\n",
    "                print(\"第【%d】步，50个预测图片误差为【%f】，准确率为【%f】\" % (i, loss_run, accuracy_run))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cnn_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
